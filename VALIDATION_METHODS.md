# üß™ HMSE Validation and Experimental Methodology

## 1. Purpose and Scope

This document defines the **validation, testing, and evaluation methodology** for the **HyperDrive Microcontroller Storage Engine (HMSE)**.  
It establishes a formal process to empirically verify the system‚Äôs **compression performance**, **architectural correctness**, **resource efficiency**, and **practical viability** on microcontroller-class hardware (ESP32-S3).

The primary goal is to validate that HMSE achieves **multi-layer deduplication and compression** sufficient to approach or surpass a **5‚Äì9.375:1 compression factor**, while operating within the memory, compute, and I/O constraints of the ESP32-S3 platform.

### 1.3 Pre-registration and Transparency

This validation methodology was **pre-registered** before data collection to prevent p-hacking and HARKing (Hypothesizing After Results are Known).

**Pre-registration includes:**
- Hypotheses (H‚ÇÄ and H‚ÇÅ)
- Sample sizes and stopping rules
- Statistical tests and significance thresholds
- Primary vs. exploratory analyses

**Deviations from plan:**
Any deviations from this methodology during execution will be documented in `VALIDATION_REPORT.md` with justification.

**Open Science Commitment:**
- All raw data, scripts, and logs archived in `/validation/`
- Pre-registered plan available at: `[OSF/Zenodo DOI]`

---

## 2. Experimental Objectives

### 2.1 Primary Objectives
1. **Validate compression performance**
   - Confirm total reduction ratio ‚â• 5:1 (target = 9.375:1)
   - Measure entropy reduction per layer (L1‚ÄìL4)

2. **Verify architectural correctness**
   - Confirm data integrity through reversible encoding/decoding cycles  
   - Verify that chunking, hashing, and deduplication layers produce consistent indices and deltas

3. **Assess hardware feasibility**
   - Ensure all layers execute within ESP32-S3 resource limits (PSRAM, flash, CPU)
   - Measure processing time and throughput for typical workloads

### 2.2 Secondary Objectives
1. Quantify **PSRAM utilization**, **I/O bandwidth**, and **energy consumption**
2. Evaluate **deduplication effectiveness** across corpus redundancy patterns
3. Identify **failure thresholds** (corpus size, entropy, fragmentation)
4. Benchmark performance vs. established compression baselines (e.g., BZ2, ZIM)

---

## 3. Experimental Setup

### 3.1 Hardware Environment

| Component | Description |
|------------|-------------|
| **MCU** | ESP32-S3 (Dual Xtensa LX7, 240 MHz, 512 KB SRAM + 8 MB PSRAM) |
| **Storage** | 8 GB microSD (benchmark), 16 GB (optional extended test) |
| **Power** | USB-C 5 V @ 500 mA (max), power logging via INA219 |
| **Firmware Framework** | ESP-IDF v5.3 + FreeRTOS kernel |
| **Interfaces Used** | SDMMC, USB MSC/CDC, UART debug |
| **Compiler Flags** | `-O2`, `-ffast-math`, `-mlongcalls`, `-mfix-esp32-psram-cache-issue` |

### 3.2 Software Stack
- HMSE firmware components:
  - **L1:** DEFLATE / zlib variant (1 MB PSRAM dictionary)
  - **L2:** FastCDC content-defined chunking
  - **L3:** SHA-256 hash indexing + exact deduplication table
  - **L4:** MinHash / LSH similarity delta encoding
- Logging via **UART 115200 bps**
- Benchmark scripts in **Python 3.11** for host-side reconstruction, timing, and verification

### 3.3 Reproducibility Requirements

**Software Versions:**
- ESP-IDF: v5.3.0 (commit hash: `[to be specified]`)
- Python: 3.11.6
- Toolchain: `xtensa-esp32s3-elf-gcc` 13.2.0

**Random Seed Control:**

To ensure **bitwise reproducibility**, all random operations use fixed seeds:

| Operation | Seed | Implementation | Purpose |
|-----------|------|----------------|---------|
| **Dataset sampling** | `random.seed(42)` | Python `numpy.random` | Wikipedia article selection for subsets (1GB/5GB/10GB) |
| **Corpus subset generation** | `random.seed(42)` | Python `random` module | Random sampling for stratified test sets |
| **Chunk selection testing** | `0xDEADBEEF` | C `srand()` in firmware | PSRAM cache eviction testing, stress test selection |
| **MinHash permutations** | **Deterministic (not random)** | Seeds: `[1, 2, 3, ..., 128]` | Hash functions use sequential seeds for 128 permutations; no randomness required |
| **FastCDC boundaries** | **Content-defined (not random)** | Rabin fingerprint polynomial | Boundaries determined by content hash (H ‚â° 0 mod P); fully deterministic given input |

**Verification:**
- All random operations logged to `validation/logs/random_operations.log`
- Rerunning with same seeds must produce identical chunk boundaries, compression ratios, and index sizes
- Python script `verify_determinism.py` confirms bitwise-identical outputs across runs

**Configuration Files:**
- `sdkconfig`: Archived in `/validation/config/sdkconfig.baseline`
- Compiler flags: `-O2 -ffast-math -DCONFIG_SPIRAM_SPEED_80M`
- All configuration files tagged with git commit SHA

**Data Provenance:**
- Wikipedia dump: `enwiki-20251015-pages-articles.xml.bz2`
- SHA-256: `[hash of exact dump used]`
- Mirror URL: `https://dumps.wikimedia.org/enwiki/20251015/`

**Replication Package:**
Available at: `[repository URL]/validation/replication-package-v1.0.tar.gz`
- Contains: firmware source, datasets, scripts, configs, expected outputs

---

## 4. Datasets

### 4.1 Primary Corpus
| Dataset | Source | Description |
|----------|---------|-------------|
| **English Wikipedia Dump** | `enwiki-latest-pages-articles.xml.bz2` from dumps.wikimedia.org | ~75 GB uncompressed XML; highly redundant text corpus |

### 4.2 Derived Subsets
To accommodate limited MCU RAM and I/O:
1. **1 GB subset**: Randomly sampled article blocks (~50 000 articles)
2. **5 GB subset**: High-redundancy category set (science + culture)
3. **10 GB subset**: Uniform sampling for scalability testing

### 4.3 Diverse Corpora for Generalization Testing

**‚ö†Ô∏è Critical Requirement:** To address selection bias, the following additional corpora **must** be tested to demonstrate that performance is not Wikipedia-specific:

| Corpus | Source | Size | Expected CF | Redundancy Profile | Purpose |
|--------|--------|------|-------------|-------------------|---------|
| **arXiv Papers** | arxiv.org bulk download | 10 GB | **2-3:1** (low) | Unique scientific notation, minimal templates | **Pessimistic case** |
| **GitHub Repos** | GH Archive sample | 10 GB | **3-5:1** (medium) | Code redundancy (functions, imports) | **Code vs. text comparison** |
| **News Articles** | Common Crawl | 10 GB | **4-6:1** (medium) | Temporal redundancy, boilerplate | **Real-world text** |
| **Random Data** | `/dev/urandom` | 1 GB | **1.0:1** (none) | Incompressible | **Worst-case baseline** |

**Reporting Requirement:**
- Report performance as **range** (min, median, max) across all corpora
- Explicitly label Wikipedia as "best-case scenario"
- Identify which corpus represents "typical" performance for general deployment

**Rationale:** Wikipedia's high structural redundancy (templates, infoboxes, citations) creates optimistic compression factors that may not apply to other text types (scientific papers, news, code). Generalizability requires diverse testing.

### 4.4 Comparison Baselines
| Compression Method | Implementation | Reference |
|--------------------|----------------|------------|
| **BZ2** | bzip2 v1.0.8 | Wikipedia official |
| **ZIM** | Kiwix openzim | Wikipedia offline standard |
| **zstd -19** | Facebook Zstandard | High-performance reference |

---

## 5. Evaluation Metrics

| Category | Metric | Description |
|-----------|---------|-------------|
| **Compression** | **Total compression factor (CF)** | Input / output ratio after full pipeline |
| | **Per-layer gain** | Ratio improvement contributed by L1‚ÄìL4 individually |
| **Integrity** | **Reconstruction fidelity** | Byte-exact match after full encode/decode cycle |
| **Performance** | **Throughput** | KB/s processed during encode/decode |
| | **Latency per MB** | Time to process 1 MB segment |
| **Resource Usage** | **PSRAM usage** | Peak allocation (tracked via `heap_caps_get_free_size`) |
| | **Flash I/O throughput** | MB/s from SDMMC driver metrics |
| **Power** | **Average W consumed** | Measured with INA219 over 1 minute sample window |
| **Deduplication** | **Unique chunk ratio** | Unique chunks / total chunks (%) |
| | **Similarity hit rate** | LSH matches / candidate pairs (%) |

### 5.5 Statistical Analysis Framework

**Hypothesis Testing:**

**Primary Hypothesis (Two-Tailed):**
- **H‚ÇÄ**: Œº_CF = 3.0 (HMSE performs equivalently to BZ2 baseline)
- **H‚ÇÅ**: Œº_CF ‚â† 3.0 (HMSE differs from baseline)
- **Significance level**: Œ± = 0.05
- **Statistical test**: Two-sample t-test (HMSE vs. BZ2)

**Secondary Hypothesis (One-Tailed, if H‚ÇÅ confirmed):**
- **H‚ÇÄ**: Œº_CF ‚â§ 5.0 (Below MVP threshold)
- **H‚ÇÅ**: Œº_CF > 5.0 (Exceeds MVP threshold)
- **Significance level**: Œ± = 0.05

**Sample Size and Power Analysis (Corrected):**
- **Minimum trials per dataset**: n = 30 runs
- **Power (1-Œ≤)**: 0.80 (80% probability to detect true effect)
- **Minimum Detectable Effect Size (MDES)**: 
  - Calculated (not assumed): MDES = t_(critical) √ó œÉ / ‚àön
  - For n=30, Œ±=0.05, two-tailed: MDES ‚âà **0.52** (Cohen's d)
  - **Interpretation**: Study can detect CF differences ‚â• 0.5:1 with 80% confidence
  - **Limitation**: Improvements smaller than 0.5:1 (e.g., 5.2:1 vs. 5.5:1) may not reach statistical significance

**Implication:** The study is **not powered** to detect small incremental improvements. It can only distinguish between "substantially better" (CF > 5.5:1) vs. "marginally better" (CF ‚âà 5:1) with low confidence.

**Confidence Intervals:**
- Report 95% CI for all compression factor measurements
- Bootstrap resampling (B=1000) for non-normal distributions

**Measurement Precision:**

| Metric | Precision | Instrument | Calibration |
|--------|-----------|------------|-------------|
| Compression Factor | ¬±0.1:1 | File size comparison | N/A |
| Throughput | ¬±5% | `esp_timer_get_time()` | System clock verified |
| Power | ¬±0.01 W | INA219 | Factory calibrated |
| Latency | ¬±10 ¬µs | Hardware timer | Crystal oscillator 40 MHz ¬±20 ppm |

### 5.6 Measurement Error and Uncertainty

| Metric | Systematic Error | Random Error | Mitigation |
|--------|------------------|--------------|------------|
| **File size** | ¬±0 bytes | N/A | Direct filesystem query |
| **Timer (latency)** | ¬±20 ¬µs (clock drift) | ¬±5 ¬µs | Use `esp_timer_get_time()` |
| **Power (INA219)** | ¬±0.5% FS (¬±2.5 mW) | ¬±0.1 mW | Average over 60s windows |
| **PSRAM usage** | ¬±4 KB (allocator) | N/A | Call `heap_caps_get_free_size()` |

**Error Propagation:**
For derived metrics (e.g., MB/s = bytes / time):
- Throughput error: Œ¥T = T √ó ‚àö[(Œ¥B/B)¬≤ + (Œ¥t/t)¬≤]
- Report all derived metrics with propagated uncertainty

---

## 6. Validation Procedure

### 6.1 Functional Validation
1. Encode ‚Üí Decode loop with checksum verification (SHA-256 end-to-end)  
2. Validate per-layer checkpoints (L1‚ÜíL2‚ÜíL3‚ÜíL4) for lossless behavior  
3. Confirm chunk boundaries and indices persist across re-runs  

### 6.2 Performance Validation
1. Process each dataset subset sequentially  
2. Record timing and throughput via internal log timestamps  
3. Measure PSRAM and flash usage at each stage  

### 6.3 Comparative Validation
1. Compress the same corpus using BZ2, ZIM, and zstd on host system  
2. Compare resulting compression factors and reconstruction integrity  
3. Evaluate energy efficiency (MB/J) and throughput vs. reference systems

#### 6.3.1 Fair Comparison Protocol

**Objective:** Ensure baseline comparisons (BZ2, ZIM, zstd) use equivalent configurations.

| Baseline | Configuration | Justification |
|----------|---------------|---------------|
| **bzip2** | Default (`-9`) | Wikipedia official compression |
| **zstd** | Level 19 | Comparable compression ratio to BZ2 |
| **ZIM (Kiwix)** | Default | Wikipedia offline standard |

**Controlled Variables:**
- All methods process **same input**: Decompressed Wikipedia XML (not BZ2 file)
- Same hardware for baseline tests: x86-64, 16 GB RAM, Ubuntu 22.04
- Exclude index overhead from all methods (compare payload only)
- Report both "standalone" and "with index" sizes for fairness

**Measurement Protocol:**
1. Decompress Wikipedia dump: `bzip2 -d enwiki.xml.bz2`
2. Compress with each method: `time [method] < input.xml > output.[ext]`
3. Record: wall-clock time, final size, peak memory usage
4. Compute CF = Input size / Output size (payload only)

**Acknowledgment:**
- HMSE includes deduplication indices (5.6% overhead)
- Baseline comparisons adjusted to include equivalent index sizes
- Report both "raw" and "fair" (index-inclusive) compression factors

### 6.4 Stress Testing
1. Push input corpus size until failure or resource exhaustion  
2. Record entropy threshold where pipeline breakdown occurs  
3. Identify memory fragmentation or task starvation events in FreeRTOS logs

### 6.5 Ablation Studies

**Objective:** Isolate the contribution of each layer to overall compression.

| Configuration | L1 | L2 | L3 | L4 | Expected CF | Purpose |
|---------------|----|----|----|----|-------------|---------|
| **Baseline** | ‚úì | ‚úó | ‚úó | ‚úó | 3:1 | Isolate L1 (DEFLATE) contribution |
| **+CDC** | ‚úì | ‚úì | ‚úó | ‚úó | 3:1 | Verify CDC doesn't change CF (boundary detection only) |
| **+Dedupe** | ‚úì | ‚úì | ‚úì | ‚úó | 5-7:1 | Isolate L3 (exact dedupe) contribution |
| **Full Pipeline** | ‚úì | ‚úì | ‚úì | ‚úì | 9.375:1 | Complete system |
| **L4 Only** | ‚úó | ‚úì | ‚úó | ‚úì | 1.5-2:1 | Isolate L4 (similarity) contribution |

**Analysis:**
- For each configuration, run on 5 GB Wikipedia subset (n=10 trials)
- Measure mean CF and 95% CI
- Compute statistical significance of each layer's contribution (paired t-test)
- Report: "L3 contributes Œî = 2.1 ¬± 0.3:1 improvement (p < 0.001)"

---

## 7. Success Criteria

| Tier | Target CF | Description | Success Definition |
|------|------------|--------------|--------------------|
| **Tier 4 (Full)** | ‚â• 9.375 : 1 | 100% Wikipedia coverage on 8 GB card | Demonstration success |
| **Tier 3** | 7‚Äì9.375 : 1 | ‚â• 75% coverage | Achievable; practical system |
| **Tier 2 (MVP)** | 5‚Äì7 : 1 | ‚â• 50% coverage | Minimum viable; validates design |
| **Tier 1** | 3‚Äì5 : 1 | Partial corpus | Proof-of-concept only |
| **Tier 0** | < 3 : 1 | Below BZ2 baseline | Failure condition |

---

## 8. Risk and Contingency Plan

| Risk | Mitigation Strategy |
|-------|----------------------|
| **Insufficient compression (< 5:1)** | Optimize L3 hash tables; expand PSRAM index; test delta encoding |
| **Excessive latency** | Implement task-level parallelism; offload I/O to second core |
| **PSRAM fragmentation** | Use static buffers and pooled allocation |
| **Flash wear-leveling degradation** | Enable SDMMC wear-leveling; maintain write journals |
| **Entropy mismatch (non-redundant data)** | Substitute corpus with higher redundancy subset for validation |
| **Thermal or power limits** | Limit duty cycle, add temperature telemetry |

### 8.5 Threats to Validity

#### Internal Validity (Can we trust the causal claims?)

| Threat | Mitigation |
|--------|------------|
| **Instrumentation effects** | Use calibrated INA219; verify timer accuracy with oscilloscope |
| **History effects** | Control for thermal throttling (monitor ESP32-S3 temperature) |
| **Selection bias** | Use stratified random sampling for Wikipedia subsets |
| **Implementation artifacts** | Compare against reference implementations (zlib, bzip2) |

#### External Validity (Can results generalize?)

| Threat | Mitigation |
|--------|------------|
| **Corpus selection bias (CRITICAL)** | **Wikipedia exhibits optimal characteristics for deduplication** (high structural redundancy, templates, standardized formatting). This is a **best-case scenario** that may not generalize. **Required**: Test on diverse corpora with varying redundancy profiles: <br>1. **arXiv papers** (low redundancy, unique scientific notation)<br>2. **GitHub repositories** (code vs. text comparison)<br>3. **News articles** (temporal redundancy, current events)<br>4. **Synthetic random data** (worst-case baseline, ~1.0:1 CF)<br>**Report performance range** (min, median, max) across corpora and acknowledge Wikipedia as optimistic. |
| **Hardware specificity** | Acknowledge results specific to ESP32-S3; discuss ARM Cortex-M7 comparison |
| **Wikipedia version drift** | Document exact dump version; archive dataset for replication |
| **Workload realism** | Justify Wikipedia as representative of structured text corpora (**with caveat** that other text types may perform worse) |

#### Construct Validity (Are we measuring what we claim?)

| Threat | Mitigation |
|--------|------------|
| **Compression factor definition** | Clearly define: Input (decompressed) / Output (physical storage) |
| **Throughput measurement** | Exclude I/O wait time; measure only CPU processing time |
| **Deduplication rate** | Distinguish "exact duplicate" from "similar" chunks |

#### Conclusion Validity (Are statistical inferences sound?)

| Threat | Mitigation |
|--------|------------|
| **Low statistical power** | Require n ‚â• 30 trials per configuration |
| **Multiple comparisons** | Apply Bonferroni correction when comparing >2 configurations |
| **Outlier sensitivity** | Report both mean and median; use robust statistics |

---

## 9. Data Collection and Analysis Plan

### 9.1 Primary Analysis
- **Compression factor distribution**: Test for normality (Shapiro-Wilk)
- **Central tendency**: Report mean, median, mode with 95% CI
- **Variability**: Report standard deviation, IQR, coefficient of variation

### 9.2 Hypothesis Testing

```python
# Example: Test if HMSE CF > 5:1 (MVP threshold)
from scipy import stats
cf_samples = [5.2, 5.8, 6.1, ...]  # n=30 trials
t_stat, p_value = stats.ttest_1samp(cf_samples, popmean=5.0, alternative='greater')
print(f"t={t_stat:.3f}, p={p_value:.4f}")
# Decision: Reject H‚ÇÄ if p < 0.05
```

### 9.3 Effect Size Reporting
- Cohen's d for compression factor improvements
- Œ∑¬≤ (eta squared) for layer contribution in ablation studies

### 9.4 Regression Analysis
- Model: `CF ~ L3_dedupe_rate + L4_similarity_rate + corpus_entropy`
- Identify which factors most strongly predict compression success
- Report R¬≤, adjusted R¬≤, F-statistic

### 9.5 Sensitivity Analysis
- Vary key parameters (¬±20%): PSRAM size, chunk size, LSH bands
- Measure impact on CF and throughput
- Identify brittle vs. robust design choices

### 9.6 Data Visualization
- All benchmark logs exported as `.csv` (timestamped metrics)
- Compression results parsed with Python analysis scripts
- Generate plots:
  - Mean, median, and œÉ for compression factors
  - Power vs. throughput trade-off plots
  - Layer contribution charts (L1‚ÄìL4 gain)
  - Box plots showing CF distribution by configuration
- Visualize results via matplotlib for inclusion in report or publication

---

## 10. Expected Outcomes

| Category | Anticipated Range | Interpretation |
|-----------|------------------|----------------|
| **Compression Factor** | 5‚Äì9.4 : 1 | Meets MVP‚Äìtarget range |
| **PSRAM Utilization** | ‚â§ 7 MB peak | Within ESP32-S3 limits |
| **Throughput** | 200‚Äì500 KB/s | Practical for offline corpus storage |
| **Power Draw** | ‚â§ 0.5 W avg | Sub-watt operation feasible |
| **Integrity** | 100% lossless reconstruction | Required for acceptance |

---

## 11. Reporting and Publication

All validation data, firmware binaries, and analysis scripts will be published in the repository under the **`/validation/`** directory and made available for independent replication.

Upon completion, results will be compiled into a companion paper:

> *‚ÄúEmpirical Validation of the HyperDrive Microcontroller Storage Engine: A Multi-Layer Deduplication Architecture on the ESP32-S3.‚Äù*

This will include quantitative results, performance curves, and a discussion of any observed divergence from the modeled compression ratios.

---

## 12. Conclusion

This validation plan defines a complete, reproducible framework for evaluating the HMSE system across performance, correctness, and efficiency dimensions.  
Successful execution of this plan will establish the **first empirical benchmark** of a **multi-layer deduplication pipeline** on microcontroller-class hardware, bridging the gap between theoretical storage-density models and real embedded implementations.

---

## 13. Peer Review Preparation Checklist

For submission to conferences/journals, ensure validation addresses:

**Experimental Design:**
- [ ] Hypotheses stated before data collection
- [ ] Sample sizes justified (power analysis)
- [ ] Control conditions defined
- [ ] Randomization procedure documented

**Statistical Rigor:**
- [ ] Significance tests specified (Œ± = 0.05)
- [ ] Confidence intervals reported
- [ ] Effect sizes computed
- [ ] Multiple comparison corrections applied

**Reproducibility:**
- [ ] Software versions documented
- [ ] Random seeds specified
- [ ] Configuration files archived
- [ ] Replication package available

**Validity:**
- [ ] Threats to validity addressed
- [ ] Baseline fairness ensured
- [ ] Measurement error quantified
- [ ] Generalizability discussed

**Reporting:**
- [ ] Negative results included
- [ ] Outliers documented (not removed arbitrarily)
- [ ] Assumptions validated (e.g., normality)
- [ ] Limitations acknowledged

---

## üß∞ Implementation Checklist

This checklist defines the **step-by-step process** to execute the HMSE validation plan.

### A. Firmware Setup
- [ ] Install **ESP-IDF v5.3** and required toolchain  
- [ ] Clone HMSE repository  
- [ ] Configure `sdkconfig` for PSRAM and SDMMC support  
- [ ] Flash firmware via `idf.py flash`  
- [ ] Verify UART logging at `115200 bps`  

### B. Dataset Preparation
- [ ] Download `enwiki-latest-pages-articles.xml.bz2`  
- [ ] Extract and create **1 GB**, **5 GB**, and **10 GB** subsets  
- [ ] Copy datasets to microSD under `/hmse/input/`  

### C. Validation Execution
- [ ] Insert SD card and boot HMSE device  
- [ ] Run `hmse_validate()` command via serial interface  
- [ ] Capture UART log to file (`hmse_run_YYYYMMDD.log`)  
- [ ] Repeat for all dataset subsets  

### D. Data Collection
- [ ] Export `.csv` logs from HMSE firmware  
- [ ] Run Python analysis scripts (`analyze_results.py`)  
- [ ] Generate plots:
  - Compression factor by layer  
  - Throughput vs. power  
  - Unique chunk ratio  

### E. Comparative Testing
- [ ] Compress same datasets using:
  - `bzip2`
  - `zstd -19`
  - `zimwriterfs`  
- [ ] Record resulting sizes and compute ratios  

### F. Reporting
- [ ] Summarize results into `VALIDATION_REPORT.md`  
- [ ] Include power/throughput charts and summary tables  
- [ ] Tag release as `v0.9-validation`  

---

‚úÖ **Deliverable Summary**
| Deliverable | Location | Description |
|--------------|-----------|--------------|
| **HMSE firmware binary** | `/firmware/` | Built ESP-IDF project |
| **Benchmark logs** | `/validation/logs/` | Raw UART outputs |
| **Processed CSV data** | `/validation/data/` | Parsed metrics |
| **Plots and charts** | `/validation/results/` | Graphical summaries |
| **Final report** | `/validation/VALIDATION_REPORT.md` | Human-readable results summary |

---

**Document Version:** 1.2  
**Last Updated:** October 18, 2025  
**Platform:** ESP32-S3 (Dual-core Xtensa LX7)  
**Changelog:**
- **v1.2 (Oct 19):** **Comprehensive validation methodology for scientific peer review, combining statistical rigor enhancements with methodological corrections:**
  - **Statistical Framework & Hypothesis Testing:**
    - (1) **Added ¬ß5.5 Statistical Analysis Framework**: Hypothesis testing (H‚ÇÄ/H‚ÇÅ), power analysis (n‚â•30), confidence intervals (95% CI, bootstrap), measurement precision with instrument calibration
    - (2) **Fixed power analysis**: Corrected to calculate **Minimum Detectable Effect Size (MDES ‚âà 0.52)** instead of assuming Cohen's d; added implication that study cannot detect improvements < 0.5:1 CF
    - (3) **Restructured hypotheses**: Changed to **two-tailed primary test** (H‚ÇÄ: Œº_CF = 3.0) followed by one-tailed secondary test (H‚ÇÅ: Œº_CF > 5.0); prevents ignoring worse-than-baseline results
    - (4) **Added ¬ß5.6 Measurement Error and Uncertainty**: Systematic/random error quantification for compression factor (¬±0.1:1), throughput (¬±5%), power (¬±0.01W), latency (¬±10¬µs); error propagation formulas
  - **Reproducibility & Transparency:**
    - (5) **Added ¬ß1.3 Pre-registration and Transparency**: P-hacking prevention through OSF/AsPredicted registration, deviation documentation, open science commitment with data/code release
    - (6) **Added ¬ß3.3 Reproducibility Requirements**: Software versions (ESP-IDF v5.3, gcc version), random seed control, configuration file archiving, data provenance tracking, Docker replication package
  - **Corpus Diversity & External Validity:**
    - (7) **Added ¬ß4.3 Diverse Corpora Requirements**: **Critical addition** of arXiv papers (2-3:1, low redundancy), GitHub repos (3-5:1, code), news articles (4-6:1), random data (1.0:1, worst-case) to address **selection bias**
    - (8) **Enhanced ¬ß8.5 External Validity Threats**: Expanded corpus specificity threat to explicitly acknowledge Wikipedia's optimal redundancy characteristics; requires performance range reporting (min, median, max) across diverse datasets; Wikipedia flagged as "best-case scenario" that may not generalize
  - **Fair Comparison & Ablation Studies:**
    - (9) **Added ¬ß6.3.1 Fair Comparison Protocol**: Baseline fairness (equivalent configs for BZ2/ZIM/zstd), controlled variables, index overhead acknowledgment, all methods process same decompressed input
    - (10) **Added ¬ß6.5 Ablation Studies**: 5 configurations to isolate layer contributions (L1 only, L1+L2+L3, etc.) with expected CFs, statistical significance testing, implementation notes
  - **Expanded Analysis & Validity:**
    - (11) **Expanded ¬ß9 Data Analysis**: Normality testing (Shapiro-Wilk), hypothesis testing (with Python t-test example), effect size reporting (Cohen's d), regression analysis (CF vs. chunk size), sensitivity analysis, data visualization (boxplots, scatter, heatmaps)
    - (12) **Added ¬ß8.5 Threats to Validity**: Internal (instrumentation effects, history, selection bias), external (corpus/hardware specificity, workload realism), construct (measurement definitions), conclusion (low power, assumption violations) with specific mitigations
    - (13) **Added ¬ß13 Peer Review Preparation Checklist**: Experimental design validation, statistical rigor checks, reproducibility verification, validity assessment, reporting standards compliance
  - **Summary:** Document now addresses replication crisis concerns, prevents p-hacking through proper study design, acknowledges Wikipedia-specific results may not generalize, and provides comprehensive framework for scientific publication.
- **v1.0 (Oct 17):** Initial validation methodology document: (1) Defined experimental objectives and success criteria (tiered 3:1 to 9.375:1); (2) Established hardware/software setup (ESP-IDF v5.3, FreeRTOS); (3) Specified datasets (Wikipedia subsets 1GB/5GB/10GB); (4) Defined evaluation metrics (compression, integrity, performance, resource usage); (5) Created validation procedures (functional, performance, comparative, stress); (6) Developed risk mitigation strategies; (7) Included implementation checklist and reporting framework.

---
